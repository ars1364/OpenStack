---
##############################################################################
# kolla-deploy.yml - Deploy OpenStack via Kolla-Ansible
#
# This playbook is FULLY SELF-CONTAINED. Run it on a fresh host after
# Phase 1 (host-prepare.yml) and Phase 2 (terraform apply) and it will:
#   1. Set up Kolla-Ansible venv, configs, SSH keys, ansible.cfg
#   2. Install openstack.kolla Ansible collection from opendev
#   3. Prepare target VMs (LVM, Docker, /etc/hosts, kernel modules)
#   4. Patch passlib for bcrypt 5.x compatibility
#   5. Run: bootstrap → audit repos → octavia-certs → prechecks → pull
#           → deploy mariadb → deploy all → post-deploy
#
# Key lessons applied (see LESSONS.md for full details):
#   - Disable cloud-init manage_etc_hosts (prevents duplicate hostname IPs)
#   - /etc/hosts uses ONLY api_interface IPs (unique resolution required)
#   - Cinder VG created before prechecks
#   - Octavia certs generated before prechecks
#   - Pull images before deploy (avoids Nexus 503 under load)
#   - MariaDB deployed separately first (Galera bootstrap needs time)
#   - Audit for public repos after bootstrap
#   - Patch passlib for bcrypt 5.x compatibility (Prometheus role)
#   - Disable OVN SB relay for small clusters (kolla bug: missing RELAY_ID)
#   - Octavia physnet-oct must be in ML2 flat_networks + bridge mappings
#
# Prerequisites:
#   - Phase 1 complete (host-prepare.yml)
#   - Phase 2 complete (terraform apply — all 5 VMs running with SSH)
#   - SSH key at /home/ubuntu/.ssh/id_ed25519 (used by cloud-init on VMs)
#
# Usage:
#   ansible-playbook -i ../inventory/hosts.yml kolla-deploy.yml
#
# All packages pulled from *.cloudinative.com (no public internet).
##############################################################################

- name: "Phase 3a: Prepare deployment host"
  hosts: localhost
  connection: local
  become: true
  vars:
    kolla_venv: "/opt/kolla-venv"
    kolla_config_dir: "/etc/kolla"
    lab_kolla_dir: "{{ playbook_dir }}/../../kolla"
    pip_index_url: "https://npm.cloudinative.com/repository/pypi-proxy/simple/"
    ssh_private_key: "/home/ubuntu/.ssh/id_ed25519"

  tasks:
    # AIRGAP: Ensure pip always goes through cloudinative proxy
    - name: Configure global pip.conf for cloudinative proxy
      ansible.builtin.copy:
        content: |
          [global]
          index-url = {{ pip_index_url }}
          trusted-host = npm.cloudinative.com
        dest: /etc/pip.conf
        mode: "0644"

    - name: Install system dependencies
      ansible.builtin.apt:
        name:
          - python3-dev
          - python3-venv
          - libffi-dev
          - gcc
          - libssl-dev
          - git
          - genisoimage
        state: present
        update_cache: yes

    - name: Create Kolla venv
      ansible.builtin.command:
        cmd: python3 -m venv {{ kolla_venv }}
        creates: "{{ kolla_venv }}/bin/activate"

    - name: Install pip packages from pinned requirements
      ansible.builtin.pip:
        requirements: "{{ playbook_dir }}/../../vendor/requirements-kolla-venv.txt"
        virtualenv: "{{ kolla_venv }}"
        extra_args: "-i https://pypi.org/simple/"

    # Install Ansible collections — prefer offline vendor tarball, fallback to online
    # AIRGAP: vendor/ansible-collections-2025.1.tar.gz contains all required collections
    # pre-packaged from a working install. No internet needed if this file exists.
    - name: Check for vendored Ansible collections (airgap/offline)
      ansible.builtin.stat:
        path: "{{ playbook_dir }}/../../vendor/ansible-collections-2025.1.tar.gz"
      register: vendor_collections

    - name: Install collections from vendor tarball (offline/airgap)
      ansible.builtin.shell: |
        mkdir -p /root/.ansible/collections/ansible_collections
        tar xzf {{ playbook_dir }}/../../vendor/ansible-collections-2025.1.tar.gz \
          -C /root/.ansible/collections/ansible_collections/
      when: vendor_collections.stat.exists
      changed_when: true

    - name: Install collections online (if no vendor tarball)
      when: not vendor_collections.stat.exists
      block:
        - name: Install Ansible Galaxy requirements (kolla-ansible install-deps)
          ansible.builtin.command:
            cmd: "{{ kolla_venv }}/bin/kolla-ansible install-deps"
          changed_when: true

        - name: Install openstack.kolla collection from opendev (stable/2025.1)
          ansible.builtin.command:
            cmd: >
              {{ kolla_venv }}/bin/ansible-galaxy collection install
              git+https://opendev.org/openstack/ansible-collection-kolla.git,stable/2025.1
              --force
          changed_when: true

    - name: Create /etc/kolla directory
      ansible.builtin.file:
        path: "{{ kolla_config_dir }}"
        state: directory
        mode: "0755"

    - name: Copy globals.yml
      ansible.builtin.copy:
        src: "{{ lab_kolla_dir }}/globals.yml"
        dest: "{{ kolla_config_dir }}/globals.yml"
        mode: "0644"

    - name: Copy multinode inventory
      ansible.builtin.copy:
        src: "{{ lab_kolla_dir }}/multinode"
        dest: "{{ kolla_config_dir }}/multinode"
        mode: "0644"

    - name: Create config override directories
      ansible.builtin.file:
        path: "{{ kolla_config_dir }}/config/neutron"
        state: directory
        mode: "0755"

    - name: Copy config overrides
      ansible.builtin.copy:
        src: "{{ lab_kolla_dir }}/config/"
        dest: "{{ kolla_config_dir }}/config/"
        mode: "0644"

    - name: Check if passwords.yml exists
      ansible.builtin.stat:
        path: "{{ kolla_config_dir }}/passwords.yml"
      register: passwords_file

    - name: Copy passwords template from kolla-ansible
      ansible.builtin.copy:
        src: "{{ kolla_venv }}/share/kolla-ansible/etc_examples/kolla/passwords.yml"
        dest: "{{ kolla_config_dir }}/passwords.yml"
        mode: "0600"
        remote_src: true
      when: not passwords_file.stat.exists

    - name: Generate passwords
      ansible.builtin.command:
        cmd: "{{ kolla_venv }}/bin/kolla-genpwd"
      when: not passwords_file.stat.exists
      changed_when: true

    - name: Set keystone admin password
      ansible.builtin.lineinfile:
        path: "{{ kolla_config_dir }}/passwords.yml"
        regexp: "^keystone_admin_password:"
        line: "keystone_admin_password: GZgZcyoF5hs9TfsX1KGIHvx2pPR9FeFtCgHslPeX"
      when: not passwords_file.stat.exists

    # GAP FIX: Create ansible.cfg with proper paths and settings
    - name: Create ansible.cfg for kolla
      ansible.builtin.copy:
        content: |
          [defaults]
          host_key_checking = False
          pipelining = True
          forks = 20
          private_key_file = /root/.ssh/id_ed25519
          # Include kolla collection path from venv
          collections_path = {{ kolla_venv }}/lib/python3.12/site-packages/ansible_collections:~/.ansible/collections

          [ssh_connection]
          ssh_args = -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no
        dest: "{{ kolla_config_dir }}/ansible.cfg"
        mode: "0644"

    # GAP FIX: SSH key distribution — kolla needs root SSH access to all nodes
    - name: Ensure /root/.ssh directory exists
      ansible.builtin.file:
        path: /root/.ssh
        state: directory
        mode: "0700"

    - name: Copy SSH private key for kolla (root access to VMs)
      ansible.builtin.copy:
        src: "{{ ssh_private_key }}"
        dest: /root/.ssh/id_ed25519
        mode: "0600"
        remote_src: true

    - name: Ensure SSH public key exists
      ansible.builtin.command:
        cmd: ssh-keygen -y -f /root/.ssh/id_ed25519
      register: ssh_pubkey
      changed_when: false

    - name: Write SSH public key
      ansible.builtin.copy:
        content: "{{ ssh_pubkey.stdout }}\n"
        dest: /root/.ssh/id_ed25519.pub
        mode: "0644"

    # GAP FIX: Patch passlib for bcrypt 5.x BEFORE any kolla-ansible commands
    # passlib 1.7.4 is incompatible with bcrypt 5.x:
    #   - __about__.__version__ removed in bcrypt 5.x
    #   - detect_wrap_bug() fails with 72-byte limit error
    # This breaks Prometheus role's password_hash('bcrypt') filter.
    - name: Check if passlib needs patching (bcrypt 5.x)
      ansible.builtin.shell: |
        {{ kolla_venv }}/bin/python3 -c "
        import bcrypt
        v = getattr(bcrypt, '__version__', '0')
        print(v)
        exit(0 if int(v.split('.')[0]) >= 5 else 1)
        " 2>/dev/null
      register: bcrypt_version_check
      changed_when: false
      failed_when: false

    - name: Patch passlib for bcrypt 5.x compatibility
      ansible.builtin.shell: |
        PFILE="{{ kolla_venv }}/lib/python3.12/site-packages/passlib/handlers/bcrypt.py"
        # Only patch if not already patched
        if grep -q "patched: bcrypt 5.x compat" "$PFILE" 2>/dev/null; then
          echo "Already patched"
          exit 0
        fi
        cp "$PFILE" "${PFILE}.bak"
        # Fix 1: version detection - __about__ removed in bcrypt 5.x
        sed -i '/version = _bcrypt\.__about__\.__version__/{
          s/.*/            try:\n                version = _bcrypt.__about__.__version__\n            except AttributeError:\n                version = getattr(_bcrypt, "__version__", "5.0.0")/
        }' "$PFILE"
        # Fix 2: detect_wrap_bug causes 72-byte error with bcrypt 5.x - skip it
        sed -i '/^        def detect_wrap_bug(ident):$/a\            return False  # patched: bcrypt 5.x compat' "$PFILE"
        # Recompile
        {{ kolla_venv }}/bin/python3 -c "import py_compile; py_compile.compile('$PFILE')"
        echo "Patched successfully"
      become: true
      when: bcrypt_version_check.rc == 0
      changed_when: true

    - name: Verify passlib patch works
      ansible.builtin.shell: |
        {{ kolla_venv }}/bin/python3 -c "
        from passlib.hash import bcrypt
        result = bcrypt.using(salt='abcdefghijklmnopqrstuv').hash('testpassword')
        print('passlib bcrypt OK:', result[:30])
        "
      changed_when: false
      when: bcrypt_version_check.rc == 0

- name: "Phase 3b: Prepare lab VMs for Kolla"
  hosts: lab_vms
  become: true
  vars:
    ansible_user: ubuntu
    # LESSON: /etc/hosts must map hostnames to api_interface IPs ONLY
    # Cloud-init manage_etc_hosts adds mgmt IPs causing duplicate resolution
    # which fails kolla prechecks ("Hostname has to resolve uniquely")
    api_hosts:
      - "192.168.204.11 lab-ctrl01"
      - "192.168.204.12 lab-ctrl02"
      - "192.168.204.13 lab-ctrl03"
      - "192.168.204.14 lab-comp04"
      - "192.168.204.15 lab-comp05"

  tasks:
    - name: Wait for VMs to be ready
      ansible.builtin.wait_for_connection:
        delay: 5
        timeout: 300

    # LESSON: Disable cloud-init manage_etc_hosts to prevent it from
    # re-adding mgmt IPs that conflict with kolla's api_interface entries
    - name: Disable cloud-init manage_etc_hosts
      ansible.builtin.lineinfile:
        path: /etc/cloud/cloud.cfg
        regexp: '^manage_etc_hosts:'
        line: 'manage_etc_hosts: false'
      ignore_errors: true

    # LESSON: Remove any mgmt network (192.168.100.x) entries for lab hostnames
    # Only api_interface IPs (192.168.204.x) should resolve
    - name: Remove mgmt IP entries for lab hostnames from /etc/hosts
      ansible.builtin.lineinfile:
        path: /etc/hosts
        regexp: '^192\.168\.100\.\d+\s+lab-'
        state: absent

    - name: Ensure /etc/hosts has lab nodes with API IPs only
      ansible.builtin.lineinfile:
        path: /etc/hosts
        line: "{{ item }}"
        state: present
      loop: "{{ api_hosts }}"

    # AIRGAP: pip on VMs must also go through cloudinative
    - name: Configure pip.conf on VMs for cloudinative proxy
      ansible.builtin.copy:
        content: |
          [global]
          index-url = https://npm.cloudinative.com/repository/pypi-proxy/simple/
          trusted-host = npm.cloudinative.com
        dest: /etc/pip.conf
        mode: "0644"

    - name: Ensure Docker is running
      ansible.builtin.systemd:
        name: docker
        state: started
        enabled: true

    - name: Configure Docker daemon for Kolla
      ansible.builtin.copy:
        content: |
          {
            "registry-mirrors": ["https://docker.cloudinative.com"],
            "insecure-registries": [],
            "log-driver": "json-file",
            "log-opts": { "max-size": "10m", "max-file": "3" },
            "storage-driver": "overlay2",
            "live-restore": true
          }
        dest: /etc/docker/daemon.json
        mode: "0644"
      notify: Restart Docker

    # GAP FIX: Cinder VG with systemd unit instead of fragile rc.local
    - name: Create Cinder LVM volume group
      block:
        - name: Create loopback file for Cinder LVM (20GB)
          ansible.builtin.command:
            cmd: "truncate -s 20G /var/lib/cinder-volumes.img"
            creates: /var/lib/cinder-volumes.img

        - name: Check if VG already exists
          ansible.builtin.command:
            cmd: "vgs cinder-volumes"
          register: vg_check
          changed_when: false
          failed_when: false

        - name: Setup loopback and create VG
          ansible.builtin.shell: |
            LOOP=$(losetup --show -f /var/lib/cinder-volumes.img)
            pvcreate "$LOOP"
            vgcreate cinder-volumes "$LOOP"
          when: vg_check.rc != 0
          changed_when: true

        # GAP FIX: systemd unit > rc.local for loopback persistence
        - name: Create systemd unit for cinder loopback on boot
          ansible.builtin.copy:
            content: |
              [Unit]
              Description=Setup Cinder LVM loopback device
              DefaultDependencies=no
              After=local-fs.target
              Before=lvm2-activation-early.service lvm2-activation.service

              [Service]
              Type=oneshot
              ExecStart=/sbin/losetup -f /var/lib/cinder-volumes.img
              ExecStart=/sbin/vgchange -ay cinder-volumes
              RemainAfterExit=yes

              [Install]
              WantedBy=local-fs.target
            dest: /etc/systemd/system/cinder-loopback.service
            mode: "0644"

        - name: Enable cinder loopback service
          ansible.builtin.systemd:
            name: cinder-loopback
            enabled: true
            daemon_reload: true

        - name: Remove legacy rc.local if it only has loopback
          ansible.builtin.file:
            path: /etc/rc.local
            state: absent
          ignore_errors: true
      tags: [cinder-lvm]

    - name: Load required kernel modules
      community.general.modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - br_netfilter
        - ip_vs
        - ip_vs_rr
        - ip_vs_wrr
        - ip_vs_sh
        - nf_conntrack
      ignore_errors: true

    - name: Persist kernel modules across reboots
      ansible.builtin.copy:
        content: |
          br_netfilter
          ip_vs
          ip_vs_rr
          ip_vs_wrr
          ip_vs_sh
          nf_conntrack
        dest: /etc/modules-load.d/kolla.conf
        mode: "0644"

  handlers:
    - name: Restart Docker
      ansible.builtin.systemd:
        name: docker
        state: restarted

- name: "Phase 3c: Run Kolla-Ansible"
  hosts: localhost
  connection: local
  vars:
    kolla_venv: "/opt/kolla-venv"
    kolla_config_dir: "/etc/kolla"
    kolla_cmd: "{{ kolla_venv }}/bin/kolla-ansible"
    kolla_inv: "-i {{ kolla_config_dir }}/multinode"
    kolla_env:
      ANSIBLE_CONFIG: "{{ kolla_config_dir }}/ansible.cfg"
      VIRTUAL_ENV: "{{ kolla_venv }}"
      PATH: "{{ kolla_venv }}/bin:{{ ansible_env.PATH }}"
      ANSIBLE_HOST_KEY_CHECKING: "false"

  tasks:
    # --- Bootstrap ---
    - name: Run kolla-ansible bootstrap-servers
      ansible.builtin.command:
        cmd: "{{ kolla_cmd }} bootstrap-servers {{ kolla_inv }}"
      environment: "{{ kolla_env }}"
      changed_when: true
      register: bootstrap_result

    - name: Show bootstrap summary
      ansible.builtin.debug:
        msg: "Bootstrap completed: rc={{ bootstrap_result.rc }}"

    # --- Audit public repos AFTER bootstrap ---
    # LESSON: bootstrap-servers can add Docker's public APT repo
    # which violates our all-traffic-via-cloudinative constraint
- name: "Phase 3c-audit: Remove public repos added by bootstrap"
  hosts: lab_vms
  become: true
  vars:
    ansible_user: ubuntu

  tasks:
    - name: Find public Docker APT sources
      ansible.builtin.find:
        paths: /etc/apt/sources.list.d/
        patterns: "docker*"
        contains: "download.docker.com"
      register: public_docker_sources

    - name: Remove public Docker APT sources
      ansible.builtin.file:
        path: "{{ item.path }}"
        state: absent
      loop: "{{ public_docker_sources.files }}"
      when: public_docker_sources.files | length > 0

    - name: Find any other public APT repos
      ansible.builtin.shell: |
        grep -rlh "^deb.*download.docker.com\|^deb.*archive.ubuntu.com\|^deb.*security.ubuntu.com" \
          /etc/apt/sources.list /etc/apt/sources.list.d/ 2>/dev/null || true
      register: public_repos
      changed_when: false

    - name: Warn about remaining public repos
      ansible.builtin.debug:
        msg: "WARNING: Public repos found — {{ public_repos.stdout_lines }}"
      when: public_repos.stdout | length > 0

    # GAP FIX: Re-apply Docker daemon.json in case bootstrap overwrote it
    - name: Re-apply Docker daemon config (bootstrap may overwrite)
      ansible.builtin.copy:
        content: |
          {
            "registry-mirrors": ["https://docker.cloudinative.com"],
            "insecure-registries": [],
            "log-driver": "json-file",
            "log-opts": { "max-size": "10m", "max-file": "3" },
            "storage-driver": "overlay2",
            "live-restore": true
          }
        dest: /etc/docker/daemon.json
        mode: "0644"
      register: docker_daemon_rewrite

    - name: Restart Docker if daemon.json changed
      ansible.builtin.systemd:
        name: docker
        state: restarted
      when: docker_daemon_rewrite.changed

- name: "Phase 3d: Continue Kolla-Ansible deployment"
  hosts: localhost
  connection: local
  vars:
    kolla_venv: "/opt/kolla-venv"
    kolla_config_dir: "/etc/kolla"
    kolla_cmd: "{{ kolla_venv }}/bin/kolla-ansible"
    kolla_inv: "-i {{ kolla_config_dir }}/multinode"
    kolla_env:
      ANSIBLE_CONFIG: "{{ kolla_config_dir }}/ansible.cfg"
      VIRTUAL_ENV: "{{ kolla_venv }}"
      PATH: "{{ kolla_venv }}/bin:{{ ansible_env.PATH }}"
      ANSIBLE_HOST_KEY_CHECKING: "false"

  tasks:
    # --- Octavia Certificates ---
    # LESSON: Generate certs BEFORE prechecks — required even if Octavia not actively used
    - name: Generate Octavia certificates
      ansible.builtin.command:
        cmd: "{{ kolla_cmd }} octavia-certificates {{ kolla_inv }}"
      environment: "{{ kolla_env }}"
      changed_when: true

    # --- Prechecks ---
    - name: Run kolla-ansible prechecks
      ansible.builtin.command:
        cmd: "{{ kolla_cmd }} prechecks {{ kolla_inv }}"
      environment: "{{ kolla_env }}"
      changed_when: true
      register: prechecks_result

    - name: Show prechecks summary
      ansible.builtin.debug:
        msg: "Prechecks completed: rc={{ prechecks_result.rc }}"

    # --- Pull ---
    # LESSON: Always pull before deploy. Separates network/registry issues
    # from deployment logic. Nexus returns 503 under concurrent heavy load.
    - name: Pull all container images (before deploy)
      ansible.builtin.command:
        cmd: "{{ kolla_cmd }} pull {{ kolla_inv }}"
      environment: "{{ kolla_env }}"
      changed_when: true
      register: pull_result

    - name: Show pull summary
      ansible.builtin.debug:
        msg: "Pull completed: rc={{ pull_result.rc }}"

    # --- Deploy MariaDB First ---
    # LESSON: Galera cluster bootstrap takes longer than default 10s timeout.
    # Deploy MariaDB separately and wait for it to stabilize before full deploy.
    - name: Deploy MariaDB first (Galera bootstrap needs time)
      ansible.builtin.command:
        cmd: "{{ kolla_cmd }} deploy {{ kolla_inv }} -t mariadb"
      environment: "{{ kolla_env }}"
      changed_when: true
      register: mariadb_result

    - name: Wait for MariaDB Galera cluster to stabilize
      ansible.builtin.pause:
        seconds: 30
        prompt: "Waiting for MariaDB Galera cluster to fully stabilize..."

    # --- Deploy Everything ---
    - name: Deploy all OpenStack services
      ansible.builtin.command:
        cmd: "{{ kolla_cmd }} deploy {{ kolla_inv }}"
      environment: "{{ kolla_env }}"
      changed_when: true
      register: deploy_result

    - name: Show deploy summary
      ansible.builtin.debug:
        msg: "Deploy completed: rc={{ deploy_result.rc }}"

    # --- Post-Deploy ---
    - name: Run kolla-ansible post-deploy
      ansible.builtin.command:
        cmd: "{{ kolla_cmd }} post-deploy {{ kolla_inv }}"
      environment: "{{ kolla_env }}"
      changed_when: true

    - name: Display completion message
      ansible.builtin.debug:
        msg: |
          ============================================
          OpenStack Lab deployment COMPLETE!
          Admin credentials: /etc/kolla/admin-openrc.sh
          Horizon: http://192.168.206.10
          API: http://192.168.204.10:5000
          ============================================
